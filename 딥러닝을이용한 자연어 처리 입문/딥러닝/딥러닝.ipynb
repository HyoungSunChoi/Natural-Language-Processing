{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 딥러닝의 학습 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) MSE\n",
    "- 연속형 변수 예측 시 사용\n",
    "- 선형 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) 이진 크로스 엔트로피\n",
    "- 로지스틱 회귀에 사용\n",
    "- 시그모이드 함수\n",
    "- binary_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Categorical Cross Entropy\n",
    "- 범주형 교차 엔트로피\n",
    "- 다중 클래스 분류 문제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) 그 외 손실 함수\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 배치(Batch size)에 따른 경사 하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) 배치 경사 하강법\n",
    "- 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려\n",
    "- 전체 데이터를 고려하기 때문에 매개 변수 업데이트에 시간이 오래걸림\n",
    "### 2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD)\n",
    "- 매개변수 값을 조정 시 랜덤의 데이터에 대하여 계산하여 빠르게 계산 가능\n",
    "### 3) 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)\n",
    "- SGD 보다 안정적\n",
    "- 2의 n 제곱 형태를 띈다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 옵티마이저(Optimizer)\n",
    "\n",
    "### 1) 모멘텀 (Momentum)\n",
    "- 관성이라는 물리학의 법칙을 응용한 방법\n",
    "- 경사 하강법에서 계산된 접선의 기울기에 한 시점 전의 접선의 기울기값을 일정한 비율만큼 반영\n",
    "- 로컬 미니멈에서 탈출하고 글로벌 미니멈 내지는 더 낮은 로컬 미니멈으로 갈 수 있는 효과\n",
    "    - tf.keras.optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "\n",
    "### 2) 아다그라드(Adagrad)\n",
    "- 모든 매개변수에 동일한 학습률(learning rate)을 적용하는 것은 비효율적\n",
    "- 아다그라드는 각 매개변수에 서로 다른 학습률을 적용\n",
    "- 변화가 많은 매개변수는 학습률이 작게 설정되고 변화가 적은 매개변수는 학습률을 높게 설정\n",
    "    - tf.keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
    "\n",
    "### 3) RMSprop\n",
    "- 아다그라드는 학습을 계속 진행한 경우에는, 나중에 가서는 학습률이 지나치게 떨어진다는 단점\n",
    "- 다른 수식으로 대체하여 단점 개선\n",
    "    - tf.keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06)\n",
    "\n",
    "### 4) 아담(Adam)\n",
    "- RMSprop, 모멘텀 두 가지를 합친 듯한 방법\n",
    "- 방향 & 학습률 두개다 잡는 방법\n",
    "    - tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 역전파(BackPropagation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. epochs and Batch size Iteration\n",
    "\n",
    "### 1) Epoch\n",
    "- 인공 신경망에서 전체 데이터에 대하여 순전파와 역전파가 끝난 상태\n",
    "- 전체 데이터를 몇번 학습하는지 정함\n",
    "\n",
    "### 2) Batch Size\n",
    "- 몇 개의 데이터 단위로 매개변수를 업데이트 하는지 정함\n",
    "- 실제값과 예측값의 오차를 계산하고, 옵티마이저가 매개변수를 업데이트 한다.\n",
    "\n",
    "### 3) Iteration / Step\n",
    "- Iteration\n",
    "    - 한 번의 에포크를 끝내기 위해 필요한 배치의 수\n",
    "    - ex) 전체 2,000, Batch가 200 이면 Iteration 은 10\n",
    "        - 한 에포크 당 10변의 매개변수 업데이트가 이루어짐\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 기울기 소실(Gradient Vanishing), 폭주(Exploding)\n",
    "- 기울기 소실\n",
    "    - 역전파 과정에서 입력층으로 갈 수록 기울기가 점차적으로 작아지는 현상\n",
    "    - 입력층에 가까운 층들에서 가중치가 업데이트가 적합하게 되지 않으면 최적의 모델을 찾을 수 없다\n",
    "- 기울기 ㄹ폭주\n",
    "    - 기울기가 커져 가중치들이 비정상적으로 큰 값이 되는 경우\n",
    "    - 순환 신경망에서 쉽게 발생한다\n",
    "### 기울기 소실/ 폭주를 막는 방법들\n",
    "\n",
    "\n",
    "### 1) 기울기 소실 완화하는 방법\n",
    "- 은닉층의 활성화 함수로 sigmoid , hyperbolictanh 대신 ReLU 나 Leaky ReLU를 사용한다\n",
    "- 은닉층에서는 시그모이드 함수를 쓰지 않는다\n",
    "- Leaky ReLU를 사용하여 기울기가 0에 수렴하지 않도록 방지한다\n",
    "- 은닉층에서는 ReLU / Leaky ReLU 를 사용하기\n",
    "\n",
    "\n",
    "### 2) 그래디언트 클리핑 (Gradient Clipping)\n",
    "- 기울기 값을 자르는 것\n",
    "    - 폭주를 막기 위해 임계값을 넘지 않도록 잘라줌\n",
    "    - 임계치만큼 크기를 감소시켜준다\n",
    "        - RNN에서는 역전파 과정에서 기울기가 커지기 때문에 활용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunny/miniforge3/envs/py38/lib/python3.8/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) 가중치 초기화(Weight initialization)\n",
    "- 가중치가 초기에 어떤 값을 가졌느냐에 따라 모델의 훈련 결과가 달라짐\n",
    "- 즉, 가중치 초기ㅐ화를 통해 기울기 소실 문제를 해결 가능\n",
    "\n",
    "#### 1) 세이비어 초기화\n",
    "- 여러 층의 기울기 분산 사이에 균형을 맞추처, 특정 층이 주목되는 것을 방지\n",
    "- Sigmoid , Hyperbolictanh 함수와 주로 활용\n",
    "    - ReLU에는 좋지않음 (He 초기화에 사용)\n",
    "\n",
    "$$W \\sim Uniform(-\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }}, +\\sqrt{\\frac{6}{ {n}_{in} + {n}_{out} }})$$\n",
    "\n",
    "- 균등 분포(Uniform Distribution)로 초기화\n",
    "- 정규 분포(Normal Distribution)으로 초기화\n",
    "    - 평균이 0, 표준 편차는 $σ=\\sqrt{\\frac { 2 }{ { n }_{ in }+{ n }_{ out } } }$\n",
    "\n",
    "#### 2) He 초기화(He initialization)\n",
    "- 세이비어 초기화와 비슷하지만, 다음 층의 뉴런의 수를 반영하지 않는 특징을 가짐\n",
    "- 뉴런의 개수를 $n_in$이라고 하고 균등 분포로 초기화할 경우 다음과 같은 균등 분포 범위를 가진다\n",
    "$$ W\\sim Uniform(- \\sqrt{\\frac { 6 }{ { n }_{ in } } } , \\space\\space + \\sqrt{\\frac { 6 }{ { n }_{ in } } } ) $$\n",
    "\n",
    "- 정규 분포로 초기화할경우 표준편차 σ 가 다음을 만족 $σ=\\sqrt{\\frac { 2 }{ { n }_{ in } } }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 배치 정규화(Batch Normalization)\n",
    "- 인공 신경망의 각 층에 들어가는 입력을 평균과 분산으로 정규화하여 학습을 효율적으로 만든다\n",
    "\n",
    "#### 1) 내부 공변량 변화(Internal Covariate Shift)\n",
    "- 학습 과정에서 층 별로 입력 데이터 분포가 달라지는 현상\n",
    "    - 이전 층들의 학습으로 가중치값이 바뀌면, 현재 층에 전달되는 입력 데이터의 분포가 현재 층이 학습했던 시점의 분포와 차이가 발생\n",
    "- 공변량 변화 : 훈련 데이터의 분포와 테스트 데이터의 분포가 다른 경우\n",
    "- 즉 신경망 층 사이에서 발생하는 입력 데이터의 분포 변화를 의미한다\n",
    "\n",
    "#### 2) 배치 정규화(Batch Normalization)\n",
    "- 배치 단위로 정규화를 한다\n",
    "    - 입력에 대해 평균을 0으로 만들고, 정규화를 진행\n",
    "    - 두 개의 매개변수 γ와 β를 사용하는데, γ는 스케일을 위해 사용하고, β는 시프트를 하는 것에 사용하며 다음 레이어에 일정한 범위의 값들만 전달함\n",
    "    - 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동 평균과 이동 분산을 저장해놓았다가 테스트 할 때는 해당 배치의 평균과 분산을 구하지 않고 구해놓았던 평균과 분산으로 정규화를 함\n",
    "- 가중치 초기화에 덜 민감해짐\n",
    "- 큰 학습률을 사용하여 학습 속도를 개선시킴\n",
    "- 미니 배치마다 평균과 표준편차를 계산하여 사용하고, 과적합을 방지함\n",
    "- 테스트 데이터에 대한 예측 시 실행 시간이 느려짐\n",
    "- 한계\n",
    "    - 미니 배치 크기에 의존적\n",
    "        - 너무 작은 배치 크기에서는 작동하지 않는다. \n",
    "        - 배치 크기가 1이면 분산은 0이 된다.\n",
    "    - RNN에 적용하기 어려움\n",
    "        - 층 정규화를 활용해야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 층 정규화(Layer Normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 시각화\n",
    "    - m이 3, 특성의 수가 4일 때의 배치 정규화\n",
    "    - 미니 배치랑 통일한 특성 개수들을 가진 다수의 샘플들을 의미!!!!!\n",
    "\n",
    "![이미지](https://wikidocs.net/images/page/61375/배치정규화.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 층 정규화 시각화\n",
    "\n",
    "![층정규화](https://wikidocs.net/images/page/61375/층정규화.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8459f5eb1ede4b4f9177267bb7d209bb99cbe04fb453024182bbd03c1d314234"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
