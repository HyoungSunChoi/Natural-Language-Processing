{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 피드 포워드 신경망 언어 모델(Feed Forward Neural Network Language Model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기존 N-gram 언어 모델의 한계\n",
    "- 언어 모델이란, \n",
    "    - 문장에 확률을 할당하는 모델\n",
    "- 언어 모델링,\n",
    "    - 주어진 문맥으로부터 아직 모르는 단어를 예측하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "# 다음 단어 예측하기\n",
    "An adorable little boy is spreading ___\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![n-gram](https://wikidocs.net/images/page/21692/n-gram.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- n-gram 언어 모델은 언어 모델링 바로 앞 n-1 개의 단어를 참고한다.\n",
    "    - 4-gram 모델은 바로 앞 3개의 단어만 참고한다.\n",
    "    - 앞의 예시는 boy, is, spreading\n",
    "    $$ P(w\\text{|boy is spreading}) = \\frac{\\text{count(boy is spreading}\\ w)}{\\text{count(boy is spreading)}} $$\n",
    "    - 갖고 있는 코퍼스에서 \"boy is spreading\"이 1000번 ,, \"boy is spreading insults\" 이 500번 ,, \"boy is spreading smiles\"가 200 번 등장한다면 각 확률은 아래와 같다\n",
    "    $$ P(\\text{insults|boy is spreading}) = 0.500$$\n",
    "    $$ P(\\text{smiles|boy is spreading}) = 0.200$$\n",
    "** 충분한 데이터를 관측히자 못하면 언어를 정확히 모델링하지 못한다 **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 단어의 의미적 유사성\n",
    "- 희소 문제는 <b>기계가</b> 단어의 의미적 유사성을 알면 해결 가능하다.\n",
    "    - \"톺아보다\"가 \"샅샅이 살펴보다\"와 유사할 때, \"발표 자료를 톺아보다\"라는 표현이 유사하므로 자연어 생성이 가능함\n",
    "    - 기계에서의 경우, \"발표 자료를 살펴보다\"는 학습하고 \"발표 자료를 톺아보다\"는 학습하지 못한 언어 모델인 경우, 언어 모델은 아래에서 다음 단어를 예측해야 한다.\n",
    "    $$P(\\text{톺아보다|발표 자료를})$$\n",
    "    $$P(\\text{냠냠하다|발표 자료를})$$\n",
    "    - 저자의 경우, \"살펴보다\"와 \"톺아보다\"의 유사성을 학습했기 때문에 둘중 톺아보다가 더 맞는 선택이라고 판단이 가능하다. 그러나 n-gram의 경우, 확률이 0이므로 고려가 불가능하다.\n",
    "        - 만약 의미적 유사성을 학습할 수 있도록 설계한다면??, 훈련 코퍼스에 없는 단어 시퀀스에 대한 예측도 가능하다.\n",
    "        - 그것이 바로 NNLM\n",
    "            - 벡터 간 유사도를 구할 수 있는 벡터를 얻어내는 워드 임베딩의 아이디어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 피드 포워드 신경망 언어 모델  (NNLM)\n",
    "- NNLM이 언어 모델링을 학습하는 과정\n",
    "    - 예시) \"what will the fat cat sit on\"\n",
    "    - 훈련 과정 -> what will the fat cat ?(예측)\n",
    "1. 훈련 코퍼스 준비\n",
    "2. 단어 수치화 (원-핫 벡터)\n",
    "```python\n",
    "what = [1, 0, 0, 0, 0, 0, 0]\n",
    "will = [0, 1, 0, 0, 0, 0, 0]\n",
    "the = [0, 0, 1, 0, 0, 0, 0]\n",
    "fat = [0, 0, 0, 1, 0, 0, 0]\n",
    "cat = [0, 0, 0, 0, 1, 0, 0]\n",
    "sit = [0, 0, 0, 0, 0, 1, 0]\n",
    "on = [0, 0, 0, 0, 0, 0, 1]\n",
    "```\n",
    "\n",
    "- NNLM 은 n-gram 모델과 다르게, 정해진 개수의 단어만 참고한다.\n",
    "    - 이를 n, 윈도우 크기를 4라고 가정하면\n",
    "    - \"what will the fat cat\"라는 단어 시퀀스가 주어졌을 때, \"will the fat cat\" 만 참고한다.\n",
    "\n",
    "![NNLM구조](https://wikidocs.net/images/page/45609/nnlm1.PNG)\n",
    "\n",
    "- 여기서 Projection layer는 행렬과의 곱셈은 이루어지지만, 활성화 함수가 존재하지 않는다.\n",
    "    - 투사층의 크기를 M으로 설정하면, 각 입력 단어들은 투사층에서 V x M 크기의 가중치 행렬과 곱해진다. 만약 원-핫 벡터의 차원이 7이고, M이 5라면, 가중치 행렬 W는 7 x 5 행렬이 된다.\n",
    "![행렬구조](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8459f5eb1ede4b4f9177267bb7d209bb99cbe04fb453024182bbd03c1d314234"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
