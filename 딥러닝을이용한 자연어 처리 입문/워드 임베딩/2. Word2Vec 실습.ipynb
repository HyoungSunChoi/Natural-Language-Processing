{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 영어 Word2Vec 만들기\n",
    "- Gensim 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) 훈련 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x289c28eb400>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 훈련 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_xml = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(target_xml)\n",
    "\n",
    "# xml 파일로부터 <content> 사이의 내용만 가져온다\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식으로 배경음 제거\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "# 입력 코퍼스를 토큰화\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장 구두점 제거 및 소문자화\n",
    "normalized_text = []\n",
    "for ss in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", ss.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 개수 : 273380\n"
     ]
    }
   ],
   "source": [
    "print(\"샘플의 개수 : {}\".format(len(result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Word2Vec 훈련시키기\n",
    "- size : 워드 벡터의 특징 값 , 임베딩 된 벡터의 차원\n",
    "- window : context 윈도우 크기\n",
    "- min_count = 단어 최소 빈도 수 제한(빈도가 적은 단어들을 학습하지 않음)\n",
    "- workers = 학습을 위한 프로세스의 수\n",
    "- sg : 0은 CBOW, 1은 Skip-gram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP\\Natural-Language-Processing\\딥러닝을이용한 자연어 처리 입문\\워드 임베딩\\2. Word2Vec 실습.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP/Natural-Language-Processing/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%20%EC%9E%85%EB%AC%B8/%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9/2.%20Word2Vec%20%EC%8B%A4%EC%8A%B5.ipynb#ch0000016?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Word2Vec, KeyedVectors\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP/Natural-Language-Processing/%EB%94%A5%EB%9F%AC%EB%8B%9D%EC%9D%84%EC%9D%B4%EC%9A%A9%ED%95%9C%20%EC%9E%90%EC%97%B0%EC%96%B4%20%EC%B2%98%EB%A6%AC%20%EC%9E%85%EB%AC%B8/%EC%9B%8C%EB%93%9C%20%EC%9E%84%EB%B2%A0%EB%94%A9/2.%20Word2Vec%20%EC%8B%A4%EC%8A%B5.ipynb#ch0000016?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39mresult, size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, sg\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\__init__.py:11\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/__init__.py?line=6'>7</a>\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m4.1.2\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/__init__.py?line=8'>9</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/__init__.py?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m parsing, corpora, matutils, interfaces, models, similarities, utils  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/__init__.py?line=13'>14</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m'\u001b[39m\u001b[39mgensim\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/__init__.py?line=14'>15</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m logger\u001b[39m.\u001b[39mhandlers:  \u001b[39m# To ensure reload() doesn't add another one\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\corpora\\__init__.py:6\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=0'>1</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=1'>2</a>\u001b[0m \u001b[39mThis package contains implementations of various streaming corpus I/O format.\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=2'>3</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=4'>5</a>\u001b[0m \u001b[39m# bring corpus classes directly into package namespace, to save some typing\u001b[39;00m\n\u001b[1;32m----> <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mindexedcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m IndexedCorpus  \u001b[39m# noqa:F401 must appear before the other classes\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmmcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m MmCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/__init__.py?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbleicorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m BleiCorpus  \u001b[39m# noqa:F401\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\corpora\\indexedcorpus.py:14\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/indexedcorpus.py?line=9'>10</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/indexedcorpus.py?line=11'>12</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/indexedcorpus.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m interfaces, utils\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/indexedcorpus.py?line=15'>16</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/corpora/indexedcorpus.py?line=18'>19</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mIndexedCorpus\u001b[39;00m(interfaces\u001b[39m.\u001b[39mCorpusABC):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\interfaces.py:19\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=6'>7</a>\u001b[0m \u001b[39m\"\"\"Basic interfaces used across the whole Gensim package.\u001b[39;00m\n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=7'>8</a>\u001b[0m \n\u001b[0;32m      <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=8'>9</a>\u001b[0m \u001b[39mThese interfaces are used for building corpora, model transformation and similarity queries.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=13'>14</a>\u001b[0m \n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=14'>15</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlogging\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m \u001b[39mimport\u001b[39;00m utils, matutils\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=21'>22</a>\u001b[0m logger \u001b[39m=\u001b[39m logging\u001b[39m.\u001b[39mgetLogger(\u001b[39m__name__\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/interfaces.py?line=24'>25</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCorpusABC\u001b[39;00m(utils\u001b[39m.\u001b[39mSaveLoad):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\matutils.py:1024\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1018'>1019</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1.\u001b[39m \u001b[39m-\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39mlen\u001b[39m(set1 \u001b[39m&\u001b[39m set2)) \u001b[39m/\u001b[39m \u001b[39mfloat\u001b[39m(union_cardinality)\n\u001b[0;32m   <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1021'>1022</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1022'>1023</a>\u001b[0m     \u001b[39m# try to load fast, cythonized code if possible\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1023'>1024</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mgensim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_matutils\u001b[39;00m \u001b[39mimport\u001b[39;00m logsumexp, mean_absolute_difference, dirichlet_expectation\n\u001b[0;32m   <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1025'>1026</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/CTLAB/anaconda3/envs/py38_NLP/lib/site-packages/gensim/matutils.py?line=1026'>1027</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mlogsumexp\u001b[39m(x):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py38_NLP\\lib\\site-packages\\gensim\\_matutils.pyx:1\u001b[0m, in \u001b[0;36minit gensim._matutils\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, size=100, window=5, min_count=5, workers=4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Word2Vec 모델 저장 & 로드 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('eng_w2v')\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 한국어 Word2Vec 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings.txt')\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값 처리\n",
    "train_data = train_data.dropna(how='any')\n",
    "\n",
    "# 정규표현식으로 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\"\")\n",
    "\n",
    "# 불용어 처리\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "# 형태소 분석기로 토큰화 작업\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True)\n",
    "    stopword_removed = [word for word in tokenized_sentence if not word in stopwords]\n",
    "    tokenized_data.append(stopword_removed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :',max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :',sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 네이버 리뷰 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences = tokenized_data, size = 100, window = 5, min_count = 5, workers = 4, sg = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 완성된 임베딩 매트릭스의 크기 확인\n",
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) 사전 훈련된 Word2Vec 임베딩\n",
    "- 감성 분류 작업할 때 데이터양이 부족한 상태라면, 사전 학습 임베딩 벡터를 가지고 온다.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "# 구글의 사전 훈련된 Word2Vec 모델을 로드.\n",
    "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
    "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word2vec_model.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 단어의 유사도 확인하기\n",
    "print(word2vec_model.similarity('this', 'is'))\n",
    "print(word2vec_model.similarity('post', 'book'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a867a277ecf2af885ec9568148caf8a8507793bb561c75f760acfb1c32ff6b3d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38_NLP')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
