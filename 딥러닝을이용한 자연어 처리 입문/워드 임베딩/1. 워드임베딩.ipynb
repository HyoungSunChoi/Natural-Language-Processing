{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 워드 임베딩 (Word Embedding)\n",
    "- 단어를 벡터로 표현하는 방법으로, 단어를 밀집 표현으로 변환한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 희소 표현(Sparse Representation)\n",
    "- 원-핫 인코딩이 대표적인 예시\n",
    "    - 해당되면 1, 아니면 0으로 표현하는 방법\n",
    "- 공간 낭비가 가장 큰 특징\n",
    "- DTM 도 비슷한 예시\n",
    "- 단어의 의미를 표현하지 못한다는 특징을 가지고 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 밀집 표현 (Dense Representation)\n",
    "- 벡터의 차원을 단어 집합의 크기로 상정하지 않는다.\n",
    "- 사용자 설정 값으로 모든 단어의 벡터 표현 차원을 맞춘다.\n",
    "    - 이때 실수값을 가지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 워드 임베딩(Word Embedding)\n",
    "- 단어를 밀집 벡터 형태로 표현하는 방법\n",
    "- 결과 -> 임베딩 벡터\n",
    "- LSA, Word2Vec, FastText, Glove 등\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 워드투벡터(Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 희소 표현\n",
    "### 2. 분산 표현\n",
    "- 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다\n",
    "### 3. CBOW(Continuous Bag of Words)\n",
    "- 주변에 있는 단어들을 입력으로 중간에 있는 단어를 예측\n",
    "    - 예측해야하는 단어 : 중심 단어(center word)\n",
    "    - 예측에 사용되는 단어 : 주변 단어(context word)\n",
    "    - 범위 : window\n",
    "    - 윈도우를 옆으로 움직임 : sliding window\n",
    "### 4. Skip-gram\n",
    "- 중간에 있는 단어들을 입력으로 주변 단어들을 예측\n",
    "- CBOW보다 성능이 좋음\n",
    "### 5. NNLM , Word2Vec\n",
    "- Word2Vec \n",
    "    - 예측 단어의 전, 후 단어들을 모두 참고\n",
    "    - 은닉층 X\n",
    "    - 학습속도 개선"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
