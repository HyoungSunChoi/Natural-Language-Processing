{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "\n",
    "- 원-핫 벡터의 벡터간 유의미한 유사도를 계산할 수 없는 문제를 해결하는 방법\n",
    "    - 한국 - 서울 + 도쿄 = 일본\n",
    "    - 박찬호 - 야구 + 축구 = 호나우두"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 희소 표현(Spare Representation)\n",
    "- 벡터/행렬의 값이 대부분 0으로 표현되는 방법\n",
    "    - 각 단어 벡터간 유의미한 유사성을 표현할 수 없는 단점이 있음\n",
    "    - 대안으로 단어의 의미를 다차원 공간에 벡터화하는 방법 -> 분산 표현(distributed representation)\n",
    "\n",
    "### 2. 분산 표현(Distributed Representation)\n",
    "- '비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다' 라는 가정을 통해 만들어짐\n",
    "    - 강아지 -> 귀엽다/예쁘다/애교 등과 함께 등장하는데, 가설에 따라 벡터화한다면 해당 단어들은 비슷한 벡터값을 가지게 된다.\n",
    "- 위와 같이 표현된 차원이 단어 집합의 크기일 필요가 없으므로, 벡터의 차원이 상대적으로 저차원으로 줄어든다.\n",
    "- 즉, 단어의 의미를 여러 차원에다가 분산하여 표현한다.\n",
    "    - 단어 벡터 간 유의미한 유사도를 계산할 수 있다.\n",
    "\n",
    "### 3. CBOW(Continuous Bag of Words)\n",
    "- Word2Vec의 학습 방식 중 첫 번째\n",
    "- CBOW : 주변에 있는 단어들을 입력으로 중간에 있는 단어들을 예측하는 방법\n",
    "    - \"The fat cat sat on the mat\"\n",
    "    - ['The','fat','cat','on','the','mat'] 으로 'sat'을 예측하는 것\n",
    "    - sat -> 중심 단어 // 배열 -> 주변 단어\n",
    "    - 중심 단어를 예측하기 위해 앞,뒤로 몇 개의 단어를 볼지 결정 : (window)\n",
    "        - 윈도우 크기가 2, 예측하는 단어가 sat 이면 ['fat','cat'], ['on','the'] 를 참고한다.\n",
    "        - 윈도우가 정해지면, 중심 단어를 바꿔가며 학습하는 데이터 셋을 만드는 것 -> 슬라이딩 윈도우\n",
    "<p align='center'><img src= 'https://wikidocs.net/images/page/22660/단어.PNG'></p>   \n",
    "\n",
    "- CBOW 인공신경망 간단하게 도식화\n",
    "    - Word2Vec은 은닉층이 다수인 딥러닝 모델이 아니라, 1개인 얕은 신경망이다.\n",
    "    - Word2Vec의 은닉층은 활성화 함수가 존재하지 않아 Projection Layer 이라고도 한다.    \n",
    "<p align='center'><img src= 'https://wikidocs.net/images/page/22660/word2vec_renew_1.PNG'></p>\n",
    "\n",
    "- Skip-Gram : 중간에 있는 단어들을 입력으로 주변 단어들을 예측하는 방법\n",
    "    - CBOW 보다 성능이 높음\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22660/word2vec_renew_6.PNG'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NNLM 보다 학습 속도에서 우위를 가지는 이유는, 은닉층을 제거하였고, 소프트맥스 & 네거티브 샘플링을 하기 때문에 학습속도가 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- NNLM : $ (n × m) + (n × m × h) + (h × V) $\n",
    "- Word2Vec : $(n × m) + (m × log(V))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
