{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
    "- 케라스의 임베딩 층(embedding layer)와 사전 훈련된 워드 임베딩(pre-trained word embedding)을 가져와 사용하는 것을 비교할 것이다\n",
    "\n",
    "- 자연어 처리를 하려고 할때, 갖고 있는 훈련 데이터의 단어들을 임베딩 층을 구현하여 임베딩 벡터로 학습한다.\n",
    "    - Keras 에서는 Embedding() 도구를 이용하여 구현한다.\n",
    "- 위키피디아 등과 같이 방대한 코퍼스를 가지고 Word2Vec, FastText, GloVe를 통해 미리 훈련된 임베딩 벡터를 불러오는 방법도 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 케라스 임베딩 층 (Keras Embedding Layer)\n",
    "- Embedding()\n",
    "\n",
    "#### 1-1) 임베딩 층은 룩업 테이블이다\n",
    "- 임베딩 층의 입력으로 사용하기 위해서는, 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 한다.\n",
    "    - 어떤 단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터\n",
    "    - 임베딩 층 : 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련된다. ( 가중치 업데이트되는 과정)\n",
    "\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/33793/lookup_table.PNG'></p>\n",
    "\n",
    "- 위 그림은 단어'great'를 정수 인코딩하여 해당 임베딩 벡터를 꺼내온다\n",
    "    - 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습된다.\n",
    "    - 단어를 정수 인코딩까지만 진행하여 임베딩 층의 입력으로 사용하여 룩업 테이블 결과인, 임베딩 벡터를 리턴해준다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 임베딩 층 구현 코드\n",
    "vocab_size = 20000\n",
    "output_dim = 128\n",
    "input_length = 500\n",
    "\n",
    "v=Embedding(vocab_size, output_dim, input_length = input_length)\n",
    "```\n",
    "\n",
    "- vocab_size : 텍스트 데이터의 전체 단어 집합의 크기\n",
    "- output_dim : 워드 임베딩 후의 임베딩 벡터의 차원\n",
    "- input_length : 입력 시퀀스의 길이, (각 샘플의 길이가 500이라면 input_length 는 500)\n",
    "\n",
    "- Embedding 으로는 (number of samples, input_length)로 2D 텐서를 input으로 입력 받고\n",
    "- output으로 (number of sammples, input_length, embedding word dimentionality) 3D텐서를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 크기 :  16\n",
      "정수 인코딩 결과 :  [[1, 2, 3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13], [14, 15]]\n",
      "최대 길이 :  4\n",
      "패딩 결과 :\n",
      "[[ 1  2  3  4]\n",
      " [ 5  6  0  0]\n",
      " [ 7  8  0  0]\n",
      " [ 9 10  0  0]\n",
      " [11 12  0  0]\n",
      " [13  0  0  0]\n",
      " [14 15  0  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 데이터 긍정(1) 부정(0)\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', \n",
    "             'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1,0,0,1,1,0,1]\n",
    "\n",
    "### 훈련 데이터 전처리\n",
    "\n",
    "# 1. 케라스의 토크나이저를 사용하여 단어 집합을 만들고 그 크기를 확인한다.\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_index) +1 # 패딩을 고려하여 +1 해준다\n",
    "print(\"단어 집합 크기 : \", vocab_size)\n",
    "\n",
    "# 2. 각 문장에 대해서 정수 인코딩 수행\n",
    "X_encoded = tokenizer.texts_to_sequences(sentences)\n",
    "print(\"정수 인코딩 결과 : \", X_encoded)\n",
    "\n",
    "# 3. 가장 길이가 긴 문장의 길이 구하기\n",
    "max_len = max(len(l) for l in X_encoded)\n",
    "print(\"최대 길이 : \", max_len)\n",
    "\n",
    "# 4. 최대 길이로 모든 샘플에 대해서 패딩 진행하기\n",
    "X_train = pad_sequences(X_encoded, maxlen=max_len, padding='post')\n",
    "y_train = np.array(y_train)\n",
    "print(\"패딩 결과 :\")\n",
    "print(X_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.6974 - acc: 0.4286\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6962 - acc: 0.4286\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6949 - acc: 0.4286\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6937 - acc: 0.4286\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6924 - acc: 0.5714\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6912 - acc: 0.5714\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6900 - acc: 0.5714\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6887 - acc: 0.5714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 23:50:49.515583: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6875 - acc: 0.5714\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6863 - acc: 0.5714\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6850 - acc: 0.5714\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6838 - acc: 0.5714\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6826 - acc: 0.7143\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6814 - acc: 0.7143\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6801 - acc: 0.7143\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6789 - acc: 0.7143\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6777 - acc: 0.7143\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6764 - acc: 0.7143\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6752 - acc: 0.8571\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6739 - acc: 0.8571\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6727 - acc: 0.8571\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6714 - acc: 0.8571\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6702 - acc: 0.8571\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6689 - acc: 0.8571\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6677 - acc: 0.8571\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6664 - acc: 0.8571\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6651 - acc: 0.8571\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6638 - acc: 0.8571\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6625 - acc: 0.8571\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6612 - acc: 0.8571\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6599 - acc: 0.8571\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6586 - acc: 0.8571\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6573 - acc: 0.8571\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6559 - acc: 0.8571\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6546 - acc: 0.8571\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6533 - acc: 0.8571\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6519 - acc: 0.8571\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6505 - acc: 0.8571\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6491 - acc: 0.8571\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6477 - acc: 0.8571\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6463 - acc: 0.8571\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6449 - acc: 0.8571\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6435 - acc: 0.8571\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6421 - acc: 0.8571\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6406 - acc: 0.8571\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6392 - acc: 0.8571\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6377 - acc: 0.8571\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6362 - acc: 0.8571\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6348 - acc: 0.8571\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6333 - acc: 0.8571\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6318 - acc: 0.8571\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6303 - acc: 0.8571\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6287 - acc: 0.8571\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6272 - acc: 0.8571\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6256 - acc: 0.8571\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6241 - acc: 0.8571\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6225 - acc: 0.8571\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6209 - acc: 0.8571\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6193 - acc: 0.8571\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6177 - acc: 0.8571\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6161 - acc: 0.8571\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6145 - acc: 0.8571\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6129 - acc: 0.8571\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6112 - acc: 0.8571\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6096 - acc: 0.8571\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6079 - acc: 0.8571\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6062 - acc: 0.8571\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6045 - acc: 0.8571\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.6029 - acc: 0.8571\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6011 - acc: 0.8571\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5994 - acc: 0.8571\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5977 - acc: 0.8571\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5960 - acc: 0.8571\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5942 - acc: 0.8571\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5925 - acc: 0.8571\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5907 - acc: 0.8571\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5889 - acc: 0.8571\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5871 - acc: 0.8571\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5853 - acc: 0.8571\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5835 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5817 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5799 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5781 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5762 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5744 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5725 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5706 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.5688 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5669 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5650 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5631 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5612 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5592 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5573 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5554 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.5534 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5515 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5495 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5476 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.5456 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2bd61c580>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 이진 분류 모델 설계\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "embedding_dim = 4\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length = max_len))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 사전 훈련된 워드 임베딩 사용하기\n",
    "- Keras의 Embedding()을 임베딩 벡터값을 학습하기도 하지만, 훈련된 워드 임베딩을 가져와서 사용하는 경우도 있다.\n",
    "- GloVe 다운로드 링크 : http://nlp.stanford.edu/data/glove.6B.zip\n",
    "- Word2Vec 다운로드 링크 : https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-1) 사전 훈련된 GloVe 사용하기\n",
    "- glove.6B.100d.txt 파일 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000개의 Embedding vector가 있습니다.\n",
      "[-0.049773   0.19903    0.10585    0.1391    -0.32395    0.44053\n",
      "  0.3947    -0.22805   -0.25793    0.49768    0.15384   -0.08831\n",
      "  0.0782    -0.8299    -0.037788   0.16772   -0.45197   -0.17085\n",
      "  0.74756    0.98256    0.81872    0.28507    0.16178   -0.48626\n",
      " -0.006265  -0.92469   -0.30625   -0.067318  -0.046762  -0.76291\n",
      " -0.0025264 -0.018795   0.12882   -0.52457    0.3586     0.43119\n",
      " -0.89477   -0.057421  -0.53724    0.25587    0.55195    0.44698\n",
      " -0.24252    0.29946    0.25776   -0.8717     0.68426   -0.05688\n",
      " -0.1848    -0.59352   -0.11227   -0.57692   -0.013593   0.18488\n",
      " -0.32507   -0.90171    0.17672    0.075601   0.54896   -0.21488\n",
      " -0.54018   -0.45882   -0.79536    0.26331    0.18879   -0.16363\n",
      "  0.3975     0.1099     0.1164    -0.083499   0.50159    0.35802\n",
      "  0.25677    0.088546   0.42108    0.28674   -0.71285   -0.82915\n",
      "  0.15297   -0.82712    0.022112   1.067     -0.31776    0.1211\n",
      " -0.069755  -0.61327    0.27308   -0.42638   -0.085084  -0.17694\n",
      " -0.0090944  0.1109     0.62543   -0.23682   -0.44928   -0.3667\n",
      " -0.21616   -0.19187   -0.032502   0.38025  ]\n",
      "벡터의 차원 수 : 100\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve, urlopen\n",
    "import gzip\n",
    "import zipfile\n",
    "\n",
    "urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", filename=\"glove.6B.zip\")\n",
    "zf = zipfile.ZipFile('glove.6B.zip')\n",
    "zf.extractall() \n",
    "zf.close()\n",
    "\n",
    "embedding_dict = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    word_vector = line.split()\n",
    "    word = word_vector[0]\n",
    "\n",
    "    # 100개의 값을 가지는 array로 변환\n",
    "    word_vector_arr = np.asarray(word_vector[1:], dtype='float32')\n",
    "    embedding_dict[word] = word_vector_arr\n",
    "f.close()\n",
    "\n",
    "print('%s개의 Embedding vector가 있습니다.' % len(embedding_dict))\n",
    "\n",
    "print(embedding_dict['respectable'])\n",
    "print('벡터의 차원 수 :',len(embedding_dict['respectable']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임베딩 행렬의 크기(shape) : (16, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "print('임베딩 행렬의 크기(shape) :',np.shape(embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('nice', 1), ('great', 2), ('best', 3), ('amazing', 4), ('stop', 5), ('lies', 6), ('pitiful', 7), ('nerd', 8), ('excellent', 9), ('work', 10), ('supreme', 11), ('quality', 12), ('bad', 13), ('highly', 14), ('respectable', 15)])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 great의 맵핑된 정수 : 2\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 great의 맵핑된 정수 :\", tokenizer.word_index['great'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.013786   0.38216    0.53236    0.15261   -0.29694   -0.20558\n",
      " -0.41846   -0.58437   -0.77355   -0.87866   -0.37858   -0.18516\n",
      " -0.128     -0.20584   -0.22925   -0.42599    0.3725     0.26077\n",
      " -1.0702     0.62916   -0.091469   0.70348   -0.4973    -0.77691\n",
      "  0.66045    0.09465   -0.44893    0.018917   0.33146   -0.35022\n",
      " -0.35789    0.030313   0.22253   -0.23236   -0.19719   -0.0053125\n",
      " -0.25848    0.58081   -0.10705   -0.17845   -0.16206    0.087086\n",
      "  0.63029   -0.76649    0.51619    0.14073    1.019     -0.43136\n",
      "  0.46138   -0.43585   -0.47568    0.19226    0.36065    0.78987\n",
      "  0.088945  -2.7814    -0.15366    0.01015    1.1798     0.15168\n",
      " -0.050112   1.2626    -0.77527    0.36031    0.95761   -0.11385\n",
      "  0.28035   -0.02591    0.31246   -0.15424    0.3778    -0.13599\n",
      "  0.2946    -0.31579    0.42943    0.086969   0.019169  -0.27242\n",
      " -0.31696    0.37327    0.61997    0.13889    0.17188    0.30363\n",
      " -1.2776     0.044423  -0.52736   -0.88536   -0.19428   -0.61947\n",
      " -0.10146   -0.26301   -0.061707   0.36627   -0.95223   -0.39346\n",
      " -0.69183   -1.0426     0.28855    0.63056  ]\n"
     ]
    }
   ],
   "source": [
    "# 사전 훈련된 GloVe의 벡터값 확인\n",
    "print(embedding_dict['great'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 단어 집합의 모든 단어에 대해서 사전 훈련된 GloVe의 임베딩 벡터들을 맵핑한 후 'great'의 벡터값이 의도한 인덱스의 위치에 삽입되었는지 확인해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, index in tokenizer.word_index.items():\n",
    "    # 단어와 맵핑되는 사전 훈련된 임베딩 벡터값\n",
    "    vector_value = embedding_dict.get(word)\n",
    "    if vector_value is not None:\n",
    "        embedding_matrix[index] = vector_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "일치\n"
     ]
    }
   ],
   "source": [
    "# 아까 great 는 인덱스가 2였기 때문에 아래와 동일하다\n",
    "if all(embedding_matrix[2] == embedding_dict['great']):\n",
    "    print(\"일치\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/1 - 0s - loss: 0.6330 - acc: 0.7143 - 174ms/epoch - 174ms/step\n",
      "Epoch 2/100\n",
      "1/1 - 0s - loss: 0.6137 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 3/100\n",
      "1/1 - 0s - loss: 0.5951 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 4/100\n",
      "1/1 - 0s - loss: 0.5774 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 5/100\n",
      "1/1 - 0s - loss: 0.5604 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 6/100\n",
      "1/1 - 0s - loss: 0.5441 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 7/100\n",
      "1/1 - 0s - loss: 0.5285 - acc: 0.7143 - 6ms/epoch - 6ms/step\n",
      "Epoch 8/100\n",
      "1/1 - 0s - loss: 0.5136 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
      "Epoch 9/100\n",
      "1/1 - 0s - loss: 0.4994 - acc: 0.8571 - 6ms/epoch - 6ms/step\n",
      "Epoch 10/100\n",
      "1/1 - 0s - loss: 0.4857 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 11/100\n",
      "1/1 - 0s - loss: 0.4726 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 12/100\n",
      "1/1 - 0s - loss: 0.4601 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 13/100\n",
      "1/1 - 0s - loss: 0.4481 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 14/100\n",
      "1/1 - 0s - loss: 0.4365 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 15/100\n",
      "1/1 - 0s - loss: 0.4254 - acc: 1.0000 - 6ms/epoch - 6ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-17 00:37:11.487519: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100\n",
      "1/1 - 0s - loss: 0.4148 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 17/100\n",
      "1/1 - 0s - loss: 0.4045 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 18/100\n",
      "1/1 - 0s - loss: 0.3946 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 19/100\n",
      "1/1 - 0s - loss: 0.3851 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 20/100\n",
      "1/1 - 0s - loss: 0.3758 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 21/100\n",
      "1/1 - 0s - loss: 0.3669 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 22/100\n",
      "1/1 - 0s - loss: 0.3583 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 23/100\n",
      "1/1 - 0s - loss: 0.3499 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 24/100\n",
      "1/1 - 0s - loss: 0.3418 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 25/100\n",
      "1/1 - 0s - loss: 0.3340 - acc: 1.0000 - 5ms/epoch - 5ms/step\n",
      "Epoch 26/100\n",
      "1/1 - 0s - loss: 0.3264 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 27/100\n",
      "1/1 - 0s - loss: 0.3190 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 28/100\n",
      "1/1 - 0s - loss: 0.3118 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 29/100\n",
      "1/1 - 0s - loss: 0.3049 - acc: 1.0000 - 9ms/epoch - 9ms/step\n",
      "Epoch 30/100\n",
      "1/1 - 0s - loss: 0.2981 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 31/100\n",
      "1/1 - 0s - loss: 0.2915 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 32/100\n",
      "1/1 - 0s - loss: 0.2851 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 33/100\n",
      "1/1 - 0s - loss: 0.2789 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 34/100\n",
      "1/1 - 0s - loss: 0.2729 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 35/100\n",
      "1/1 - 0s - loss: 0.2670 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 36/100\n",
      "1/1 - 0s - loss: 0.2613 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 37/100\n",
      "1/1 - 0s - loss: 0.2558 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 38/100\n",
      "1/1 - 0s - loss: 0.2504 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 39/100\n",
      "1/1 - 0s - loss: 0.2452 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 40/100\n",
      "1/1 - 0s - loss: 0.2401 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 41/100\n",
      "1/1 - 0s - loss: 0.2352 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 42/100\n",
      "1/1 - 0s - loss: 0.2304 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 43/100\n",
      "1/1 - 0s - loss: 0.2257 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 44/100\n",
      "1/1 - 0s - loss: 0.2212 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 45/100\n",
      "1/1 - 0s - loss: 0.2168 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 46/100\n",
      "1/1 - 0s - loss: 0.2125 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 47/100\n",
      "1/1 - 0s - loss: 0.2083 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 48/100\n",
      "1/1 - 0s - loss: 0.2043 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 49/100\n",
      "1/1 - 0s - loss: 0.2004 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 50/100\n",
      "1/1 - 0s - loss: 0.1965 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 51/100\n",
      "1/1 - 0s - loss: 0.1928 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 52/100\n",
      "1/1 - 0s - loss: 0.1892 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 53/100\n",
      "1/1 - 0s - loss: 0.1857 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 54/100\n",
      "1/1 - 0s - loss: 0.1823 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 55/100\n",
      "1/1 - 0s - loss: 0.1790 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 56/100\n",
      "1/1 - 0s - loss: 0.1757 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 57/100\n",
      "1/1 - 0s - loss: 0.1726 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 58/100\n",
      "1/1 - 0s - loss: 0.1695 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 59/100\n",
      "1/1 - 0s - loss: 0.1665 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 60/100\n",
      "1/1 - 0s - loss: 0.1636 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 61/100\n",
      "1/1 - 0s - loss: 0.1608 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 62/100\n",
      "1/1 - 0s - loss: 0.1581 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 63/100\n",
      "1/1 - 0s - loss: 0.1554 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 64/100\n",
      "1/1 - 0s - loss: 0.1528 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 65/100\n",
      "1/1 - 0s - loss: 0.1502 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 66/100\n",
      "1/1 - 0s - loss: 0.1478 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 67/100\n",
      "1/1 - 0s - loss: 0.1453 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 68/100\n",
      "1/1 - 0s - loss: 0.1430 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 69/100\n",
      "1/1 - 0s - loss: 0.1407 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 70/100\n",
      "1/1 - 0s - loss: 0.1385 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 71/100\n",
      "1/1 - 0s - loss: 0.1363 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 72/100\n",
      "1/1 - 0s - loss: 0.1341 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 73/100\n",
      "1/1 - 0s - loss: 0.1321 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 74/100\n",
      "1/1 - 0s - loss: 0.1300 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 75/100\n",
      "1/1 - 0s - loss: 0.1281 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 76/100\n",
      "1/1 - 0s - loss: 0.1261 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 77/100\n",
      "1/1 - 0s - loss: 0.1242 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 78/100\n",
      "1/1 - 0s - loss: 0.1224 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 79/100\n",
      "1/1 - 0s - loss: 0.1206 - acc: 1.0000 - 10ms/epoch - 10ms/step\n",
      "Epoch 80/100\n",
      "1/1 - 0s - loss: 0.1188 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 81/100\n",
      "1/1 - 0s - loss: 0.1171 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 82/100\n",
      "1/1 - 0s - loss: 0.1154 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 83/100\n",
      "1/1 - 0s - loss: 0.1138 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 84/100\n",
      "1/1 - 0s - loss: 0.1122 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 85/100\n",
      "1/1 - 0s - loss: 0.1106 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 86/100\n",
      "1/1 - 0s - loss: 0.1091 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 87/100\n",
      "1/1 - 0s - loss: 0.1076 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 88/100\n",
      "1/1 - 0s - loss: 0.1061 - acc: 1.0000 - 14ms/epoch - 14ms/step\n",
      "Epoch 89/100\n",
      "1/1 - 0s - loss: 0.1047 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 90/100\n",
      "1/1 - 0s - loss: 0.1033 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 91/100\n",
      "1/1 - 0s - loss: 0.1019 - acc: 1.0000 - 8ms/epoch - 8ms/step\n",
      "Epoch 92/100\n",
      "1/1 - 0s - loss: 0.1006 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 93/100\n",
      "1/1 - 0s - loss: 0.0993 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 94/100\n",
      "1/1 - 0s - loss: 0.0980 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 95/100\n",
      "1/1 - 0s - loss: 0.0967 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 96/100\n",
      "1/1 - 0s - loss: 0.0955 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 97/100\n",
      "1/1 - 0s - loss: 0.0943 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 98/100\n",
      "1/1 - 0s - loss: 0.0931 - acc: 1.0000 - 6ms/epoch - 6ms/step\n",
      "Epoch 99/100\n",
      "1/1 - 0s - loss: 0.0919 - acc: 1.0000 - 7ms/epoch - 7ms/step\n",
      "Epoch 100/100\n",
      "1/1 - 0s - loss: 0.0908 - acc: 1.0000 - 6ms/epoch - 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d571f130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "output_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "e = Embedding(vocab_size, output_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "model.add(e)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "model.fit(X_train, y_train, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10b2c5cc0d681fc2a82a570e3a34207b13b3f3d3d10bb22b3d9fcfe94c3d5d7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
