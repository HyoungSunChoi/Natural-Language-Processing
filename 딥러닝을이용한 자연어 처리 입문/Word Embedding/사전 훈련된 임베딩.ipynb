{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사전 훈련된 워드 임베딩(Pre-trained Word Embedding)\n",
    "- 케라스의 임베딩 층(embedding layer)와 사전 훈련된 워드 임베딩(pre-trained word embedding)을 가져와 사용하는 것을 비교할 것이다\n",
    "\n",
    "- 자연어 처리를 하려고 할때, 갖고 있는 훈련 데이터의 단어들을 임베딩 층을 구현하여 임베딩 벡터로 학습한다.\n",
    "    - Keras 에서는 Embedding() 도구를 이용하여 구현한다.\n",
    "- 위키피디아 등과 같이 방대한 코퍼스를 가지고 Word2Vec, FastText, GloVe를 통해 미리 훈련된 임베딩 벡터를 불러오는 방법도 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 케라스 임베딩 층 (Keras Embedding Layer)\n",
    "- Embedding()\n",
    "\n",
    "#### 1-1) 임베딩 층은 룩업 테이블이다\n",
    "- 임베딩 층의 입력으로 사용하기 위해서는, 입력 시퀀스의 각 단어들은 모두 정수 인코딩이 되어있어야 한다.\n",
    "    - 어떤 단어 -> 단어에 부여된 고유한 정수값 -> 임베딩 층 통과 -> 밀집 벡터\n",
    "    - 임베딩 층 : 입력 정수에 대해 밀집 벡터(dense vector)로 맵핑하고 이는 인공 신경망의 학습 과정에서 가중치가 학습되는 것과 같은 방식으로 훈련된다. ( 가중치 업데이트되는 과정)\n",
    "\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/33793/lookup_table.PNG'></p>\n",
    "\n",
    "- 위 그림은 단어'great'를 정수 인코딩하여 해당 임베딩 벡터를 꺼내온다\n",
    "    - 임베딩 벡터는 모델의 입력이 되고, 역전파 과정에서 단어 great의 임베딩 벡터값이 학습된다.\n",
    "    - 단어를 정수 인코딩까지만 진행하여 임베딩 층의 입력으로 사용하여 룩업 테이블 결과인, 임베딩 벡터를 리턴해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임베딩 층 구현 코드\n",
    "vocab_size = 20000\n",
    "output_dim = 128\n",
    "input_length = 500"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10b2c5cc0d681fc2a82a570e3a34207b13b3f3d3d10bb22b3d9fcfe94c3d5d7f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
