{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 글로브(GloVe) \n",
    "#### Global Vectors for Word Representation\n",
    "\n",
    "- 카운트 기반과 예측 기반을 모두 사용하는 방법론\n",
    "- 카운트 기반(LSA), 예측 기반(Word2Vec)의 단점을 보완"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 기존 방법론에 대한 비판\n",
    "- LSA\n",
    "    - DTM, TF-IDF 행렬과 같이 각 문서에서의 각 단어의 빈도수를 카운트 한 행렬이라는 전체적인 통계 정보를 입려긍로 받아 차원을 축소(Truncated SVD)하여 잠재된 의미를 끌어내는 방법론\n",
    "    - 코퍼스의 전체적인 통계 정보를 고려하지만, \"왕:남자 = 여왕:? \" 와 같은 단어 의미의 유추 작업에는 성능이 떨어진다.\n",
    "- Word2Vec\n",
    "    - 실제값과 예측값에 대한 오차를 손실 함수를 통해 줄여나가, 학습하는 예측 기반의 방법론\n",
    "    - 유추작업은 LSA에 비해 뛰어나지만, 임베딩 벡터가 윈도우 크기 내에서만 주변 단어를 고려하기 때문에, 코퍼스의 전체적인 통계 정보를 반영하지 못한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 윈도우 기반 동시 등장 행렬\n",
    "##### Window based Co-occurrence Matrix\n",
    "- 단어의 동시 등장 행렬은 행과 열을 전체 단어 집합의 단어들로 구성하고, i 단어의 윈도우 크기내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
    "\n",
    "- I like deep learning\n",
    "- I like NLP\n",
    "- I enjoy flying\n",
    "![](2022-02-13-16-49-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.동시 등장 확률(Co-occurence Probability)\n",
    "- 어떤 동시 등장 행렬을 가지고 정리한 동시 등장 확률이다\n",
    "- $ 동시 등장 확률 P(k|i)$는 동시 등장행렬로부터 특정 단어 i의 전체 등장 횟수를 카운트하고, 특정 단어 i가 등장했을 때, 어떤 단어 k 가 등장한 횟수를 카운트하여 계산한 조건부 확률이다.\n",
    "\n",
    "- $P(k|i)$ 에서 i를 중심단어, k를 주변단어라고 했을때, 중심 단어i의 행의 모든 값을 더한 값을 분모, k열을 분자로 한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](2022-02-13-16-54-14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 손실함수 (Loss Function)\n",
    "- $X$ : 동시 등장 행렬(Co-occurrence Matrix)\n",
    "- $X_ij$ : 중심 단어 i 가 뜽장했을 때, 윈도우 내 주변 단어 j 가 등장하는 횟수\n",
    "- $X_{i} : \\sum_j X_{ij}$ : 동시 등장 행렬에서 i 행의 값을 모두 더한 값\n",
    "- $ P_{ik} : P(k\\ |\\ i) = \\frac{X_{ik}}{X_i}$ : 중심 단어 i가 등장했을 때 윈도우 내 주변 단어 k 가 등장할 확률\n",
    "    - 예) P(solid|ice) = 단어 ice가 등장했을 때, 단어 solid가 등장할 확률\n",
    "- $ \\frac{P_{ik}}{P_{jk}} : P_{ik}를 P_{jk}로 나눠준 값$ \n",
    "    - 예) P(solidc|ice) / P(solid|steam) = 8.9\n",
    "- $ w_i $ : 중심 단어 i의 임베딩 벡터\n",
    "- $ \\tilde{w_{k}} $ : 주변 단어 k의 임베딩 벡터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서, 동시 등장 확률이 되도록 만드는 것. \n",
    "$$dot\\ product(w_{i}\\ \\tilde{w_{k}}) \\approx\\ log\\ P(k\\ |\\ i) = log\\ P_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8459f5eb1ede4b4f9177267bb7d209bb99cbe04fb453024182bbd03c1d314234"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
