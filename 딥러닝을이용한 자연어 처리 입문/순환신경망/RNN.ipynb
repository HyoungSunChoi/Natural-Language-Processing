{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 순환 신경망(Recurrent Neural Network)\n",
    "- RNN\n",
    "- 입력과 출력을 시퀀스 단위로 처리하는 시퀀스(Sequence) 모델\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 복습 **\n",
    "- 피드 포워드 신경망(Feed Forward Neural Network, FFNN)\n",
    "    - 은기층에서 활성화 함수를 지난 값은 출력층 방향으로만 향하는 특징"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. RNN\n",
    "- 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산의 입력으로 보내는 특징\n",
    "\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image1_ver2.PNG'></p>\n",
    "\n",
    "\n",
    "- x는 입력층의 입력 벡터\n",
    "- y는 출력층의 출력 벡터\n",
    "- cell은 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할의 노드\n",
    "    - 이전의 값을 기억하려는 일종의 메모리 역할을 수행한다\n",
    "    - 메모리 셀 / RNN 셀이라고 표현\n",
    "- 은닉층에서의 각 시점에서는 바로 이전 시점에서의 출력값을 입력으로 사용한다 (재귀적 활동)\n",
    "- 은닉 상태(hidden state) : 메모리 셀이 다음 시점에게 보내는 값\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image2_ver3.PNG'></p>\n",
    "\n",
    "- t 시점의 은닉 상태 계산의 입력값을 (t-1)의 메모리 셀이 보낸 은닉 상태값으로 사용한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image2.5.PNG'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 차원 4, 은닉 상태 2, 출력 차원 2이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN 설계 방법\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image3_ver2.PNG'></p>\n",
    "\n",
    "- 일 대 다 구조\n",
    "    - 하나의 입력에 대해서 여러개의 출력을 하는 모델\n",
    "    - 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝 작업에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다 대 일 구조\n",
    "    - 단어 시퀀스에 대해서 하나의 출력을 하는 모델\n",
    "    - 입력 문서가 긍정적/부정적 판별하는 감성 분류\n",
    "    - 메일 정상/스팸 메일 판별하는 스팸 메일 분류에 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다 대 다 구조\n",
    "    - 사용자가 문장을 입력하면 대답을 출력하는 챗봇, 번역기, 태깅 작업에 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN 수식 이해하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image4_ver2.PNG'></p>\n",
    "\n",
    "- $h_t$ : 현재 시점 t에서의 은닉 상태값\n",
    "    - 입력층을 위한 가중치 $W_x$, t-1의 은닉 상태값인 $h_{t-1}$ 을 위한 가중치 $W_h$\n",
    "    - 즉, $W_h + W_X$\n",
    "\n",
    "\n",
    "- 은닉층 : $h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)$\n",
    "- 출력층 : $y_{t} = f(W_{y}h_{t} + b)$ <br>단, $f$는 비선형 활성화 함수 중 하나\n",
    "\n",
    "- 자연어 처리에서 $x_t$ 는 단어 벡터로 간주된다. 이때 단어 벡터의 차원을 d라고 하고, 은닉 상태의 크기를 $D_h$라고 했을 떄, 각 벡터와 행렬의 크기는 다음과 같다\n",
    "    - $x_t : ( d \\times 1)$\n",
    "    - $W_x : (D_h \\times d)$\n",
    "    - $W_h : (D_h \\times D_h)$\n",
    "    - $h_{t-1} : (D_h \\times 1)$\n",
    "    - $b : (D_h \\times 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 크기가 1이고, $d와 D_h$ 두 값을 모두 4로 가정했을 때, 연산은 다음과 같다\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_images4-5.PNG'></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras 로 RNN 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "model.add(SimpleRNN(hidden_units))\n",
    "# 추가 인자를 사용할 때\n",
    "model.add(SimpleRNN(hiddne_units, input_shape=(timesteps, input_dim)))\n",
    "# 다른 표기\n",
    "model.add(SimpleRNN(hidden_units, input_length = M, input_dim = N))\n",
    "```\n",
    "\n",
    "- hidden units \n",
    "    - 은닉 상태의 크기를 정의, 메모리 셀이 다음 시점의 메모리 셀과 출력층으로 보내는 값의 크기가 동일\n",
    "    - 용량(capacity)을 늘리면, 128, 256, 512, 1024 이렇다.\n",
    "- timesteps \n",
    "    - 입력 시퀀스의 길이(Input_length), \n",
    "- input_dim \n",
    "    - 입력의 크기\n",
    "\n",
    "- RNN 은 (batch_size, timesteps, input_dim) 크기의 3D 텐서를 입력으로 받는다.\n",
    "    - batch_size : 한번에 학습하는 데이터의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 입력 3D 텐서를 받아 은닉 상태 출력하는 방법\n",
    "    - 최종 시점의 은닉 상태만 리턴 : (batch_size, output_dim) \n",
    "        - 다 대 일 문제 해결 가능\n",
    "    - 메모리 셀의 각 시점(time step)의 은닉 상태값을 모아 전체 시퀀스 리턴 :<br> (batch_size, timesteps, output_dim) \n",
    "        - 다 대 다 문제 해결 가능\n",
    "        - return_sequences=True 로 설정 가능\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image8_ver2.PNG'></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (None, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n출력값이 (batch_size, output_dim) 크기의 2D 텐서일 때, output_dim은 hidden_units의 값인 3입니다. 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됩니다. 이번에는 batch_size를 미리 정의해보겠습니다.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "model.summary()\n",
    "\"\"\"\n",
    "출력값이 (batch_size, output_dim) 크기의 2D 텐서일 때, output_dim은 hidden_units의 값인 3입니다. 이 경우 batch_size를 현 단계에서는 알 수 없으므로 (None, 3)이 됩니다. 이번에는 batch_size를 미리 정의해보겠습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_3 (SimpleRNN)    (8, 3)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'batch_size를 8로 기재하면 출력의 크기가 (8, 3)이 됩니다. return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴하도록 모델을 만들어 보겠습니다.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10)))\n",
    "model.summary()\n",
    "\"\"\"batch_size를 8로 기재하면 출력의 크기가 (8, 3)이 됩니다. return_sequences 매개 변수에 True를 기재하여 출력값으로 (batch_size, timesteps, output_dim) 크기의 3D 텐서를 리턴하도록 모델을 만들어 보겠습니다.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (8, 2, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 깊은 순환 신경망(Deep Recurrent Neural Network)\n",
    "\n",
    "```python\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_units, input_length=10, input_dim=5, return_sequences=True))\n",
    "model.add(SimpleRNN(hidden_units, return_sequences=True))\n",
    "```\n",
    "\n",
    "- 의 코드에서 첫번째 은닉층은 다음 은닉층이 존재하므로 return_sequences = True를 설정하여 모든 시점에 대해서 은닉 상태 값을 다음 은닉층으로 보내주고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 양방향 순환 신경망 (Bidirectional Recurrent Neural Network)\n",
    "\n",
    "<p align='center'><img src='https://wikidocs.net/images/page/22886/rnn_image6_ver3.PNG'></p>\n",
    "\n",
    "- 시점 t에서의 출력값 예측시, 이전 시점 입력 + 이후 시점 입력을 사용\n",
    "- 과거의 시점, 미래 시점의 입력에 힌트를 얻어낸다\n",
    "- 양방향 RNN은 하나의 출력값을 예측하기 위해 두 개의 메모리 셀을 사용한다.\n",
    "    - 첫번째 메모리 셀은 앞 시점의 은닉 상태를 전달받아 현재의 은닉 상태를 계산(주황색 메모리 셀)\n",
    "    - 두번쨰 메모리 셀은 뒤 시점의 은닉 상태를 전달 받아 현재 은닉 상태를 계산 (초록색 메모리 셀)\n",
    "- 예시\n",
    "    - 운동을 열심히 하는 것은 [ $\\dots$  ]을 늘리는데 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 제대로 이해했는지 퀴즈를 통해서 확인해보세요! 모델에 대한 설명이 다음과 같을 때, 총 파라미터 개수를 구해보세요.\n",
    "\n",
    "- Embedding을 사용하며, 단어 집합(Vocabulary)의 크기가 5,000이고 임베딩 벡터의 차원은 100입니다.\n",
    "- 은닉층에서는 Simple RNN을 사용하며, 은닉 상태의 크기는 128입니다.\n",
    "- 훈련에 사용하는 모든 샘플의 길이는 30으로 가정합니다.\n",
    "- 이진 분류를 수행하는 모델로, 출력층의 뉴런은 1개로 시그모이드 함수를 사용합니다.\n",
    "- 은닉층은 1개입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 100)         500000    \n",
      "                                                                 \n",
      " simple_rnn_5 (SimpleRNN)    (None, 128)               29312     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 529,441\n",
      "Trainable params: 529,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "vocab, vector = 5000,100\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab,vector))\n",
    "model.add(SimpleRNN(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8459f5eb1ede4b4f9177267bb7d209bb99cbe04fb453024182bbd03c1d314234"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
