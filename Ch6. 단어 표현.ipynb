{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리에서의 단어 표현\n",
    "자연어 처리를 위한 모델에 적용할 수 있게 언어적인 특성을 반영해서 단어를 수치화하는 방법.\n",
    "- 단어 임베딩\n",
    "\n",
    "- 단어 벡터\n",
    "    - 원 핫 인코딩\n",
    "        - 각 단어를 0, 1 값만 가지는 벡터로 표현\n",
    "        - ex) 아빠, 엄마, 나 -> [1,0,0], [0,1,0], [0,0,1]\n",
    "        - 용량이 큰 경우 비효율적, 단어의 의미나 특성을 전혀 표현할 수 없음\n",
    "    - 분포 가설 (Distributed hypothesis)\n",
    "        - 같은 문맥의 단어, 비슷한 위치에 나오는 단어는 비슷한 의미를 가진다\n",
    "        - 특정 문맥 안에서 단어들이 동시 등장하는 횟수를 직접 세는 카운트 기반 방법\n",
    "            - 동시 등장하는 횟수 : 동시 출현/ 공기 (Co-occurrence)\n",
    "            - 동시 등장 횟수를 하나의 행렬로 나타낸 뒤 그 행렬을 수치화하여 단어 벡터로 만든다\n",
    "            - 특이값 분해 (Singular Value Decomposition, SVD)\n",
    "            - 잠재의미분석 (Latent Semantic Analysis, LSA)\n",
    "            - Hyperspace Analogue to Language, HAL)\n",
    "            - Hellinger PCA (Principal Component Analysis)\n",
    "            \n",
    "            \n",
    "        - 신경망 등을 통해 문맥 안의 단어들을 예측하는 방법인 예측 방법\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 예측방법\n",
    "- Word2vec\n",
    "    - CBOW(Continuous Bag of Words)\n",
    "        - 어떤 단어를 문맥 안의 주변 단어들을 통해 예측하는 방법\n",
    "        - 창욱은 냉장고에서 ___ 꺼내어 먹었다.\n",
    "        - 1. 각 주변 단어들을 원-핫 벡터로 만들어 입력값으로 사용(입력층 벡터)\n",
    "        - 2. 가중치 행렬(weight matrix)를 각 원-핫 벡터에 곱하여 n-차원 벡터를 만든다 (N-차원 은닉충)\n",
    "        - 3. 만들어진 n-차원 벡터를 모두 더한 후 개수로 나눠 평균 n-차원 벡터를 만든다.(출력층 벡터)\n",
    "        - 4. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다.\n",
    "        - 5. 만들어진 벡터를 실제 예측하려고 하는 단어의 원-핫 벡터와 비교하여 학습한다.\n",
    "    - Skip-Gram\n",
    "        -어떤 단어를 가지고 특정 문맥 안의 주변 단어들을 예측하는 방법\n",
    "        - ___ ___ 음식을 ___ ___\n",
    "        - 1. 하나의 단어를 원-핫 벡터로 만들어 입력값으로 사용한다 ( 입력층 벡터)\n",
    "        - 2. 가중치 행렬을 원-핫 벡터에 곱해서 n-차원 벡터를 만든다 (N-차원 은닉충)\n",
    "        - 3. n-차원 벡터에 다시 가중치 행렬을 곱해서 원-핫 벡터와 같은 차원의 벡터로 만든다(출력층 벡터)\n",
    "        - 4. 만들어진 벡터를 실제 예측하려는 주변 단어들 각각의 원-핫 벡터와 비교하여 학습한다.\n",
    "- NNLM(Neural Network Language Model)\n",
    "\n",
    "- RNNLM(Recurrent Neural Network Language Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
